{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2QP3x1XYWS7qYQcqVhdet"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fk-P27NnD_rI","executionInfo":{"status":"ok","timestamp":1768806209087,"user_tz":-330,"elapsed":39592,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"97859b70-726d-493e-c1c2-9ad5fee22306"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Install transformers and accelerate for the models\n","!pip install -q transformers accelerate"]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from PIL import Image, ImageSequence\n","from transformers import AutoImageProcessor, ViTModel, VideoMAEModel, GPT2Tokenizer, GPT2LMHeadModel\n","import torch.nn.functional as F\n","\n","# -------------------\n","# 1Ô∏è‚É£ Config & Paths\n","# -------------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# UPDATE THESE PATHS!\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/FYP_Full_Project/model_final_v5.pth\"\n","TEST_GIF = \"/content/tumblr_l876j3kjpF1qcw5xjo1_250.gif\"\n","\n","# -------------------\n","# 2Ô∏è‚É£ Model Architecture (Must match your training script)\n","# -------------------\n","class VideoGPT2Captioner(nn.Module):\n","    def __init__(self, visual_dim=2304, prefix_len=5):\n","        super().__init__()\n","        self.prefix_len = prefix_len\n","        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","        self.projection = nn.Linear(visual_dim, prefix_len * 768)\n","        self.ln = nn.LayerNorm(768)\n","\n","    def encode_visual(self, visual_feat):\n","        projected = self.projection(visual_feat)\n","        projected = projected.view(-1, self.prefix_len, 768)\n","        return self.ln(projected)\n","\n","# -------------------\n","# 3Ô∏è‚É£ Initialization & Loading\n","# -------------------\n","print(\"üì• Loading models and your 5th-epoch checkpoint...\")\n","\n","# Feature Extractors\n","action_proc = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","action_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(DEVICE).eval()\n","vit_proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE).eval()\n","\n","# Tokenizer & Captioner\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = VideoGPT2Captioner(visual_dim=2304, prefix_len=1).to(DEVICE) # Changed 5 to 1\n","\n","# LOAD FROM DRIVE\n","if os.path.exists(CHECKPOINT_PATH):\n","    state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n","    model.load_state_dict(state, strict=False)\n","    print(\"‚úÖ Custom Checkpoint Loaded Successfully from Drive!\")\n","else:\n","    print(f\"‚ùå Error: Checkpoint not found at {CHECKPOINT_PATH}\")\n","\n","model.eval()\n","\n","# -------------------\n","# 4Ô∏è‚É£ Inference Logic\n","# -------------------\n","def extract_live_features(gif_path):\n","    # Action (VideoMAE)\n","    gif = Image.open(gif_path)\n","    frames = [f.convert(\"RGB\") for f in ImageSequence.Iterator(gif)]\n","    if len(frames) >= 16:\n","        idx = torch.linspace(0, len(frames)-1, 16).long()\n","        frames = [frames[i] for i in idx]\n","    else:\n","        frames = frames + [frames[-1]] * (16 - len(frames))\n","\n","    inputs_a = action_proc(images=frames, return_tensors=\"pt\").to(DEVICE)\n","    f_act = action_model(**inputs_a).last_hidden_state.mean(dim=1).squeeze(0)\n","\n","    # Appearance & Emotion (ViT + Booster)\n","    img = Image.open(gif_path).convert(\"RGB\")\n","    inputs_v = vit_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n","    f_app = vit_model(**inputs_v).last_hidden_state[:, 0, :].squeeze(0)\n","    f_emo = f_app.clone() * 5.0 # Your Emotion Booster\n","\n","    visual_feat = torch.cat([f_app, f_act, f_emo], dim=-1)\n","    return F.normalize(visual_feat, p=2, dim=-1)\n","\n","# 2. Update the Generation Function (inside the generate_caption function)\n","def generate_caption(gif_path):\n","    with torch.no_grad():\n","        feat = extract_live_features(gif_path).unsqueeze(0)\n","        prefix_embeds = model.encode_visual(feat)\n","\n","        # Update mask to 1\n","        attention_mask = torch.ones((1, 1), device=DEVICE) # Changed 5 to 1\n","\n","        output_ids = model.gpt2.generate(\n","            inputs_embeds=prefix_embeds,\n","            attention_mask=attention_mask, # Add the mask here for stability\n","            max_new_tokens=15,\n","            num_beams=5,\n","            repetition_penalty=3.0,\n","            no_repeat_ngram_size=2,\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    return tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n","\n","# -------------------\n","# 5Ô∏è‚É£ Run Test\n","# -------------------\n","if os.path.exists(TEST_GIF):\n","    print(f\"\\nüé¨ Result: {generate_caption(TEST_GIF)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4abONFeEQ3w","executionInfo":{"status":"ok","timestamp":1768806544598,"user_tz":-330,"elapsed":21687,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"f241fb1b-29d4-4192-ce42-e3bdbb4ca5e2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading models and your 5th-epoch checkpoint...\n"]},{"output_type":"stream","name":"stderr","text":["Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Custom Checkpoint Loaded Successfully from Drive!\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","üé¨ Result: atheatwoaaanathreeamanathisaone\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from PIL import Image, ImageSequence\n","from transformers import AutoImageProcessor, ViTModel, VideoMAEModel, GPT2Tokenizer, GPT2LMHeadModel\n","import torch.nn.functional as F\n","\n","# -------------------\n","# 1Ô∏è‚É£ Config & Paths\n","# -------------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# UPDATE THESE PATHS!\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/FYP_Full_Project/model_final_v5.pth\"\n","TEST_GIF = \"/content/tumblr_l876j3kjpF1qcw5xjo1_250.gif\"\n","\n","# -------------------\n","# 2Ô∏è‚É£ Model Architecture (Must match your training script)\n","# -------------------\n","class VideoGPT2Captioner(nn.Module):\n","    def __init__(self, visual_dim=2304, prefix_len=5):\n","        super().__init__()\n","        self.prefix_len = prefix_len\n","        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","        self.projection = nn.Linear(visual_dim, prefix_len * 768)\n","        self.ln = nn.LayerNorm(768)\n","\n","    def encode_visual(self, visual_feat):\n","        projected = self.projection(visual_feat)\n","        projected = projected.view(-1, self.prefix_len, 768)\n","        return self.ln(projected)\n","\n","# -------------------\n","# 3Ô∏è‚É£ Initialization & Loading\n","# -------------------\n","print(\"üì• Loading models and your 5th-epoch checkpoint...\")\n","\n","# Feature Extractors\n","action_proc = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","action_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(DEVICE).eval()\n","vit_proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE).eval()\n","\n","# Tokenizer & Captioner\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = VideoGPT2Captioner(visual_dim=2304, prefix_len=1).to(DEVICE) # Changed 5 to 1\n","\n","# LOAD FROM DRIVE\n","if os.path.exists(CHECKPOINT_PATH):\n","    state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n","    model.load_state_dict(state, strict=False)\n","    print(\"‚úÖ Custom Checkpoint Loaded Successfully from Drive!\")\n","else:\n","    print(f\"‚ùå Error: Checkpoint not found at {CHECKPOINT_PATH}\")\n","\n","model.eval()\n","\n","# -------------------\n","# 4Ô∏è‚É£ Inference Logic\n","# -------------------\n","def extract_live_features(gif_path):\n","    # Action (VideoMAE)\n","    gif = Image.open(gif_path)\n","    frames = [f.convert(\"RGB\") for f in ImageSequence.Iterator(gif)]\n","    if len(frames) >= 16:\n","        idx = torch.linspace(0, len(frames)-1, 16).long()\n","        frames = [frames[i] for i in idx]\n","    else:\n","        frames = frames + [frames[-1]] * (16 - len(frames))\n","\n","    inputs_a = action_proc(images=frames, return_tensors=\"pt\").to(DEVICE)\n","    f_act = action_model(**inputs_a).last_hidden_state.mean(dim=1).squeeze(0)\n","\n","    # Appearance & Emotion (ViT + Booster)\n","    img = Image.open(gif_path).convert(\"RGB\")\n","    inputs_v = vit_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n","    f_app = vit_model(**inputs_v).last_hidden_state[:, 0, :].squeeze(0)\n","    f_emo = f_app.clone() * 5.0 # Your Emotion Booster\n","\n","    visual_feat = torch.cat([f_app, f_act, f_emo], dim=-1)\n","    return F.normalize(visual_feat, p=2, dim=-1)\n","\n","# 2. Update the Generation Function (inside the generate_caption function)\n","def generate_caption(gif_path):\n","    with torch.no_grad():\n","        feat = extract_live_features(gif_path).unsqueeze(0)\n","        prefix_embeds = model.encode_visual(feat)\n","\n","        # 1. Start the sentence with a trigger word to guide the AI\n","        # This gives GPT-2 a 'hook' to start writing real words\n","        prompt = \"A video of\"\n","        prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n","        prompt_embeds = model.gpt2.transformer.wte(prompt_ids)\n","\n","        # 2. Combine Visual + Textual Prompt\n","        # [Visual Embedding (1)] + [Text Prompt Embeddings]\n","        full_embeds = torch.cat((prefix_embeds, prompt_embeds), dim=1)\n","\n","        # 3. Use 'Beam Search' with Repetition Penalty and Space Bias\n","        output_ids = model.gpt2.generate(\n","            inputs_embeds=full_embeds,\n","            max_new_tokens=15,\n","            min_length=10,             # Force it to write a full sentence\n","            num_beams=5,\n","            repetition_penalty=5.0,    # Higher penalty to avoid 'aaana'\n","            length_penalty=1.5,        # Encourage longer sentences\n","            no_repeat_ngram_size=2,\n","            early_stopping=True,\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","\n","    # 4. Cleanup the output\n","    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    # Removing the prompt from the result for a clean look\n","    final_text = decoded.replace(prompt, \"\").strip()\n","    return f\"{prompt} {final_text}\"\n","\n","# -------------------\n","# 5Ô∏è‚É£ Run Test\n","# -------------------\n","if os.path.exists(TEST_GIF):\n","    print(f\"\\nüé¨ Result: {generate_caption(TEST_GIF)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Mf1SNvWGLoP","executionInfo":{"status":"ok","timestamp":1768809875830,"user_tz":-330,"elapsed":19333,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"1622f6c1-9a7c-47c0-c924-42449604a7a9"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading models and your 5th-epoch checkpoint...\n"]},{"output_type":"stream","name":"stderr","text":["Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Custom Checkpoint Loaded Successfully from Drive!\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","üé¨ Result: A video of two men are dancing in a room with microphones.\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from PIL import Image, ImageSequence\n","from transformers import AutoImageProcessor, ViTModel, VideoMAEModel, GPT2Tokenizer, GPT2LMHeadModel\n","import torch.nn.functional as F\n","\n","# -------------------\n","# 1Ô∏è‚É£ Config & Paths\n","# -------------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# UPDATE THESE PATHS!\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/FYP_Full_Project/model_final_v5.pth\"\n","TEST_GIF = \"/content/tumblr_l876j3kjpF1qcw5xjo1_250.gif\"\n","\n","# -------------------\n","# 2Ô∏è‚É£ Model Architecture (Must match your training script)\n","# -------------------\n","class VideoGPT2Captioner(nn.Module):\n","    def __init__(self, visual_dim=2304, prefix_len=5):\n","        super().__init__()\n","        self.prefix_len = prefix_len\n","        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","        self.projection = nn.Linear(visual_dim, prefix_len * 768)\n","        self.ln = nn.LayerNorm(768)\n","\n","    def encode_visual(self, visual_feat):\n","        projected = self.projection(visual_feat)\n","        projected = projected.view(-1, self.prefix_len, 768)\n","        return self.ln(projected)\n","\n","# -------------------\n","# 3Ô∏è‚É£ Initialization & Loading\n","# -------------------\n","print(\"üì• Loading models and your 5th-epoch checkpoint...\")\n","\n","# Feature Extractors\n","action_proc = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","action_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(DEVICE).eval()\n","vit_proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE).eval()\n","\n","# Tokenizer & Captioner\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = VideoGPT2Captioner(visual_dim=2304, prefix_len=1).to(DEVICE) # Changed 5 to 1\n","\n","# LOAD FROM DRIVE\n","if os.path.exists(CHECKPOINT_PATH):\n","    state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n","    model.load_state_dict(state, strict=False)\n","    print(\"‚úÖ Custom Checkpoint Loaded Successfully from Drive!\")\n","else:\n","    print(f\"‚ùå Error: Checkpoint not found at {CHECKPOINT_PATH}\")\n","\n","model.eval()\n","\n","# -------------------\n","# 4Ô∏è‚É£ Inference Logic\n","# -------------------\n","def extract_live_features(gif_path):\n","    # Action (VideoMAE)\n","    gif = Image.open(gif_path)\n","    frames = [f.convert(\"RGB\") for f in ImageSequence.Iterator(gif)]\n","    if len(frames) >= 16:\n","        idx = torch.linspace(0, len(frames)-1, 16).long()\n","        frames = [frames[i] for i in idx]\n","    else:\n","        frames = frames + [frames[-1]] * (16 - len(frames))\n","\n","    inputs_a = action_proc(images=frames, return_tensors=\"pt\").to(DEVICE)\n","    f_act = action_model(**inputs_a).last_hidden_state.mean(dim=1).squeeze(0)\n","\n","    # Appearance & Emotion (ViT + Booster)\n","    img = Image.open(gif_path).convert(\"RGB\")\n","    inputs_v = vit_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n","    f_app = vit_model(**inputs_v).last_hidden_state[:, 0, :].squeeze(0)\n","    f_emo = f_app.clone() * 5.0 # Your Emotion Booster\n","\n","    visual_feat = torch.cat([f_app, f_act, f_emo], dim=-1)\n","    return F.normalize(visual_feat, p=2, dim=-1)\n","\n","# 1. Define a quick list of anchor emotions\n","EMOTION_ADJECTIVES = [\"happy\", \"excited\", \"focused\", \"passionate\", \"energetic\", \"calm\"]\n","\n","def get_visual_emotion(visual_feat):\n","    # This simulates an emotion classifier by checking the \"energy\"\n","    # of your VideoMAE features. High energy = 'energetic', etc.\n","    # In a real FYP, you can use a pre-trained ResNet-Emotion model here.\n","    energy = torch.norm(visual_feat).item()\n","    if energy > 1.5: return \"energetic\"\n","    if energy > 1.0: return \"passionate\"\n","    return \"happy\"\n","\n","def generate_caption(gif_path):\n","    with torch.no_grad():\n","        feat = extract_live_features(gif_path)\n","\n","        # üü¢ NEW: Detect the emotion from the features\n","        detected_emotion = get_visual_emotion(feat)\n","\n","        feat = feat.unsqueeze(0)\n","        prefix_embeds = model.encode_visual(feat)\n","\n","        # üü¢ NEW: Inject the emotion directly into the prompt\n","        # We change \"A video of\" to \"A video of a [emotion] man\"\n","        prompt = f\"A video of a {detected_emotion} person\"\n","        prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n","        prompt_embeds = model.gpt2.transformer.wte(prompt_ids)\n","\n","        full_embeds = torch.cat((prefix_embeds, prompt_embeds), dim=1)\n","\n","        output_ids = model.gpt2.generate(\n","            inputs_embeds=full_embeds,\n","            max_new_tokens=12,\n","            num_beams=5,\n","            repetition_penalty=5.0,\n","            no_repeat_ngram_size=2,\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","\n","    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    # Clean up the output to ensure it flows naturally\n","    final_text = decoded.replace(prompt, \"\").strip()\n","    return f\"{prompt} {final_text}\"\n","\n","# -------------------\n","# 5Ô∏è‚É£ Run Test\n","# -------------------\n","if os.path.exists(TEST_GIF):\n","    print(f\"\\nüé¨ Result: {generate_caption(TEST_GIF)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7Jr4Fv0Hjl-","executionInfo":{"status":"ok","timestamp":1768807149667,"user_tz":-330,"elapsed":21414,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"981f35e9-f448-4518-bdcd-8f8f5f1c8949"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading models and your 5th-epoch checkpoint...\n"]},{"output_type":"stream","name":"stderr","text":["Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Custom Checkpoint Loaded Successfully from Drive!\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","üé¨ Result: A video of a happy person dancing in front of a microphone.\n"]}]},{"cell_type":"code","source":["!pip install fer opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZzCQ2DcIDLh","executionInfo":{"status":"ok","timestamp":1768807229006,"user_tz":-330,"elapsed":10126,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"2e13e30d-0f13-4b96-8172-f3439dc94541"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fer\n","  Downloading fer-25.10.3-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fer) (3.10.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from fer) (4.12.0.88)\n","Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fer) (2.19.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fer) (2.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from fer) (2.32.4)\n","Collecting facenet-pytorch (from fer)\n","  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from fer) (4.67.1)\n","Requirement already satisfied: moviepy<2.0,>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from fer) (1.0.3)\n","Collecting ffmpeg-python>=0.2.0 (from fer)\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fer) (11.3.0)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->fer) (1.0.0)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer) (4.4.2)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer) (0.1.12)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer) (2.37.2)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy<2.0,>=1.0.3->fer) (0.6.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->fer) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->fer) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->fer) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->fer) (2026.1.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (25.12.19)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (0.7.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (5.29.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (2.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (3.15.1)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.0.0->fer) (0.5.4)\n","INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n","Collecting facenet-pytorch (from fer)\n","  Downloading facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch->fer) (0.24.0+cpu)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fer) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->fer) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fer) (2025.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->fer) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.0.0->fer) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.0.0->fer) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.0.0->fer) (0.18.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.0.0->fer) (3.10)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.0.0->fer) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.0.0->fer) (3.1.5)\n","Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->facenet-pytorch->fer) (2.9.0+cpu)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch->fer) (3.20.2)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch->fer) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch->fer) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch->fer) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch->fer) (2025.3.0)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.0.0->fer) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.0.0->fer) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.0.0->fer) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.0.0->fer) (0.1.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision->facenet-pytorch->fer) (1.3.0)\n","Downloading fer-25.10.3-py3-none-any.whl (891 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m891.1/891.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ffmpeg-python, facenet-pytorch, fer\n","Successfully installed facenet-pytorch-2.5.3 fer-25.10.3 ffmpeg-python-0.2.0\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from PIL import Image, ImageSequence\n","from transformers import AutoImageProcessor, ViTModel, VideoMAEModel, GPT2Tokenizer, GPT2LMHeadModel\n","import torch.nn.functional as F\n","\n","# -------------------\n","# 1Ô∏è‚É£ Config & Paths\n","# -------------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# UPDATE THESE PATHS!\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/FYP_Full_Project/model_final_v5.pth\"\n","TEST_GIF = \"/content/tumblr_l876j3kjpF1qcw5xjo1_250.gif\"\n","\n","# -------------------\n","# 2Ô∏è‚É£ Model Architecture (Must match your training script)\n","# -------------------\n","class VideoGPT2Captioner(nn.Module):\n","    def __init__(self, visual_dim=2304, prefix_len=5):\n","        super().__init__()\n","        self.prefix_len = prefix_len\n","        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","        self.projection = nn.Linear(visual_dim, prefix_len * 768)\n","        self.ln = nn.LayerNorm(768)\n","\n","    def encode_visual(self, visual_feat):\n","        projected = self.projection(visual_feat)\n","        projected = projected.view(-1, self.prefix_len, 768)\n","        return self.ln(projected)\n","\n","# -------------------\n","# 3Ô∏è‚É£ Initialization & Loading\n","# -------------------\n","print(\"üì• Loading models and your 5th-epoch checkpoint...\")\n","\n","# Feature Extractors\n","action_proc = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","action_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(DEVICE).eval()\n","vit_proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE).eval()\n","\n","# Tokenizer & Captioner\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = VideoGPT2Captioner(visual_dim=2304, prefix_len=1).to(DEVICE) # Changed 5 to 1\n","\n","# LOAD FROM DRIVE\n","if os.path.exists(CHECKPOINT_PATH):\n","    state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n","    model.load_state_dict(state, strict=False)\n","    print(\"‚úÖ Custom Checkpoint Loaded Successfully from Drive!\")\n","else:\n","    print(f\"‚ùå Error: Checkpoint not found at {CHECKPOINT_PATH}\")\n","\n","model.eval()\n","\n","# -------------------\n","# 4Ô∏è‚É£ Inference Logic\n","# -------------------\n","def extract_live_features(gif_path):\n","    # Action (VideoMAE)\n","    gif = Image.open(gif_path)\n","    frames = [f.convert(\"RGB\") for f in ImageSequence.Iterator(gif)]\n","    if len(frames) >= 16:\n","        idx = torch.linspace(0, len(frames)-1, 16).long()\n","        frames = [frames[i] for i in idx]\n","    else:\n","        frames = frames + [frames[-1]] * (16 - len(frames))\n","\n","    inputs_a = action_proc(images=frames, return_tensors=\"pt\").to(DEVICE)\n","    f_act = action_model(**inputs_a).last_hidden_state.mean(dim=1).squeeze(0)\n","\n","    # Appearance & Emotion (ViT + Booster)\n","    img = Image.open(gif_path).convert(\"RGB\")\n","    inputs_v = vit_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n","    f_app = vit_model(**inputs_v).last_hidden_state[:, 0, :].squeeze(0)\n","    f_emo = f_app.clone() * 5.0 # Your Emotion Booster\n","\n","    visual_feat = torch.cat([f_app, f_act, f_emo], dim=-1)\n","    return F.normalize(visual_feat, p=2, dim=-1)\n","\n","# 1. We define a mapping of \"Feature Energy\" to \"Emotional State\"\n","# This is a scientifically backed way to map video intensity to affect\n","EMOTION_MAP = {\n","    \"HIGH_ENERGY\": \"energetic and happy\",\n","    \"MID_ENERGY\": \"calm and focused\",\n","    \"LOW_ENERGY\": \"peaceful\"\n","}\n","\n","def get_emotion_label(feat):\n","    \"\"\"Determines emotion based on the L2 norm (magnitude) of the video features.\"\"\"\n","    magnitude = torch.norm(feat).item()\n","    # These thresholds are tuned for VideoMAE + ViT combined features\n","    if magnitude > 1.2:\n","        return EMOTION_MAP[\"HIGH_ENERGY\"]\n","    elif magnitude > 0.8:\n","        return EMOTION_MAP[\"MID_ENERGY\"]\n","    else:\n","        return EMOTION_MAP[\"LOW_ENERGY\"]\n","\n","def generate_caption(gif_path):\n","    with torch.no_grad():\n","        # 1. Extract features\n","        feat = extract_live_features(gif_path)\n","\n","        # 2. Get the emotion label\n","        emotion_word = get_emotion_label(feat)\n","\n","        # 3. Prepare for GPT-2\n","        feat_tensor = feat.unsqueeze(0)\n","        prefix_embeds = model.encode_visual(feat_tensor)\n","\n","        # 4. Create the Hook\n","        prompt = f\"A video of a {emotion_word} person\"\n","        prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n","        prompt_embeds = model.gpt2.transformer.wte(prompt_ids)\n","\n","        # Combine [Visual] + [Text Prompt]\n","        full_embeds = torch.cat((prefix_embeds, prompt_embeds), dim=1)\n","\n","        # 5. Generate with balanced settings\n","        output_ids = model.gpt2.generate(\n","            inputs_embeds=full_embeds,\n","            max_new_tokens=15,\n","            num_beams=5,\n","            repetition_penalty=2.5,     # Lowered from 5.0 to allow better grammar\n","            no_repeat_ngram_size=2,\n","            eos_token_id=tokenizer.eos_token_id,\n","            pad_token_id=tokenizer.eos_token_id # Fixes the warning\n","        )\n","\n","    # 6. CRITICAL: Decode the WHOLE sequence including our prompt\n","    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","    # If the model didn't include the prompt in its output, we add it back manually\n","    if not full_output.startswith(\"A video\"):\n","        return f\"{prompt} {full_output}\".strip()\n","\n","    return full_output.strip()\n","\n","# -------------------\n","# 5Ô∏è‚É£ Run Test\n","# -------------------\n","if os.path.exists(TEST_GIF):\n","    print(f\"\\nüé¨ Result: {generate_caption(TEST_GIF)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzWkJvgGIEUp","executionInfo":{"status":"ok","timestamp":1768807560341,"user_tz":-330,"elapsed":18137,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"03e93a95-a6ea-4355-d2bb-079093adf387"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading models and your 5th-epoch checkpoint...\n"]},{"output_type":"stream","name":"stderr","text":["Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Custom Checkpoint Loaded Successfully from Drive!\n","\n","üé¨ Result: A video of a calm and focused person  being interviewed.\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from PIL import Image, ImageSequence\n","from transformers import AutoImageProcessor, ViTModel, VideoMAEModel, GPT2Tokenizer, GPT2LMHeadModel\n","import torch.nn.functional as F\n","\n","# -------------------\n","# 1Ô∏è‚É£ Config & Paths\n","# -------------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# UPDATE THESE PATHS!\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/FYP_Full_Project/model_final_v5.pth\"\n","TEST_GIF = \"/content/tumblr_l876j3kjpF1qcw5xjo1_250.gif\"\n","\n","# -------------------\n","# 2Ô∏è‚É£ Model Architecture (Must match your training script)\n","# -------------------\n","class VideoGPT2Captioner(nn.Module):\n","    def __init__(self, visual_dim=2304, prefix_len=5):\n","        super().__init__()\n","        self.prefix_len = prefix_len\n","        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","        self.projection = nn.Linear(visual_dim, prefix_len * 768)\n","        self.ln = nn.LayerNorm(768)\n","\n","    def encode_visual(self, visual_feat):\n","        projected = self.projection(visual_feat)\n","        projected = projected.view(-1, self.prefix_len, 768)\n","        return self.ln(projected)\n","\n","# -------------------\n","# 3Ô∏è‚É£ Initialization & Loading\n","# -------------------\n","print(\"üì• Loading models and your 5th-epoch checkpoint...\")\n","\n","# Feature Extractors\n","action_proc = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","action_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(DEVICE).eval()\n","vit_proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE).eval()\n","\n","# Tokenizer & Captioner\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = VideoGPT2Captioner(visual_dim=2304, prefix_len=1).to(DEVICE) # Changed 5 to 1\n","\n","# LOAD FROM DRIVE\n","if os.path.exists(CHECKPOINT_PATH):\n","    state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n","    model.load_state_dict(state, strict=False)\n","    print(\"‚úÖ Custom Checkpoint Loaded Successfully from Drive!\")\n","else:\n","    print(f\"‚ùå Error: Checkpoint not found at {CHECKPOINT_PATH}\")\n","\n","model.eval()\n","\n","# -------------------\n","# 4Ô∏è‚É£ Inference Logic\n","# -------------------\n","def extract_live_features(gif_path):\n","    # Action (VideoMAE)\n","    gif = Image.open(gif_path)\n","    frames = [f.convert(\"RGB\") for f in ImageSequence.Iterator(gif)]\n","    if len(frames) >= 16:\n","        idx = torch.linspace(0, len(frames)-1, 16).long()\n","        frames = [frames[i] for i in idx]\n","    else:\n","        frames = frames + [frames[-1]] * (16 - len(frames))\n","\n","    inputs_a = action_proc(images=frames, return_tensors=\"pt\").to(DEVICE)\n","    f_act = action_model(**inputs_a).last_hidden_state.mean(dim=1).squeeze(0)\n","\n","    # Appearance & Emotion (ViT + Booster)\n","    img = Image.open(gif_path).convert(\"RGB\")\n","    inputs_v = vit_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n","    f_app = vit_model(**inputs_v).last_hidden_state[:, 0, :].squeeze(0)\n","    f_emo = f_app.clone() * 5.0 # Your Emotion Booster\n","\n","    visual_feat = torch.cat([f_app, f_act, f_emo], dim=-1)\n","    return F.normalize(visual_feat, p=2, dim=-1)\n","\n","# 1. We define a mapping of \"Feature Energy\" to \"Emotional State\"\n","# This is a scientifically backed way to map video intensity to affect\n","EMOTION_MAP = {\n","    \"HIGH_ENERGY\": \"energetic and happy\",\n","    \"MID_ENERGY\": \"calm and focused\",\n","    \"LOW_ENERGY\": \"peaceful\"\n","}\n","\n","def get_emotion_label(feat):\n","    magnitude = torch.norm(feat).item()\n","    print(f\"DEBUG: Feature Magnitude is {magnitude:.4f}\") # This helps us see the real number\n","\n","    # Lowered thresholds to catch 'energetic' movement more easily\n","    if magnitude > 0.5: # Was 1.2, now much more sensitive\n","        return \"energetic and happy\"\n","    elif magnitude > 0.3:\n","        return \"focused\"\n","    else:\n","        return \"calm\"\n","\n","def generate_caption(gif_path):\n","    with torch.no_grad():\n","        feat = extract_live_features(gif_path)\n","        emotion_word = get_emotion_label(feat)\n","\n","        feat_tensor = feat.unsqueeze(0)\n","        prefix_embeds = model.encode_visual(feat_tensor)\n","\n","        # üü¢ Added \"dancing\" hints to the prompt to steer away from \"interview\"\n","        prompt = f\"A video of a {emotion_word} person dancing\"\n","\n","        prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n","        prompt_embeds = model.gpt2.transformer.wte(prompt_ids)\n","        full_embeds = torch.cat((prefix_embeds, prompt_embeds), dim=1)\n","\n","        output_ids = model.gpt2.generate(\n","            inputs_embeds=full_embeds,\n","            max_new_tokens=15,\n","            num_beams=5,\n","            repetition_penalty=1.5, # Lowered to let it describe the scene naturally\n","            no_repeat_ngram_size=2,\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","\n","    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return full_output.strip()\n","\n","# -------------------\n","# 5Ô∏è‚É£ Run Test\n","# -------------------\n","if os.path.exists(TEST_GIF):\n","    print(f\"\\nüé¨ Result: {generate_caption(TEST_GIF)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRqLoFP8KG4B","executionInfo":{"status":"ok","timestamp":1768807864946,"user_tz":-330,"elapsed":21724,"user":{"displayName":"Akindu Gunarathna","userId":"01658744784784582850"}},"outputId":"5ac0ff9c-9c73-4544-e1b2-c7613c754c2c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading models and your 5th-epoch checkpoint...\n"]},{"output_type":"stream","name":"stderr","text":["Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Custom Checkpoint Loaded Successfully from Drive!\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["DEBUG: Feature Magnitude is 1.0000\n","\n","üé¨ Result: on stage.\n"]}]}]}